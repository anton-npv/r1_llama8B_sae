{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e473e7dc",
   "metadata": {},
   "source": [
    "# 02 ‑ Fine‑Tune SAE for DeepSeek R1‑Distill\n",
    "This notebook fine‑tunes Goodfire’s SAE on R1‑distill layer 19 activations using a \n",
    "reasoning‑heavy dataset (R1 traces) blended with a small sample of LMSYS chat data.\n",
    "After training, we evaluate reconstruction fidelity and save the adapted SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sae-lens transformers accelerate datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load models & SAE\n",
    "r1_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "sae_repo = 'Goodfire/Llama-3.1-8B-Instruct-SAE-l19'\n",
    "sae_id   = 'blocks.19.hook_resid_post'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r1_model_name)\n",
    "r1_model  = AutoModelForCausalLM.from_pretrained(r1_model_name, device_map='auto')\n",
    "r1_model.eval()\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(release=sae_repo, sae_id=sae_id, device=device)\n",
    "print('SAE loaded. Latent dim =', sae_cfg['d_sae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820745b4",
   "metadata": {},
   "source": [
    "## Load reasoning and chat datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff227ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load R1 reasoning traces (example dataset name)\n",
    "reason_ds = load_dataset('phunguyen01/open-r1-math-220k', split='train[:10%]')\n",
    "\n",
    "# Load subset of LMSYS chat for generality\n",
    "chat_ds = load_dataset('lmsys/lmsys-chat-1m', split='train[:2%]')\n",
    "\n",
    "# Combine with 80/20 weighting (already controlled by slice ratios above)\n",
    "combined_texts = [ex['text'] for ex in reason_ds]\n",
    "combined_texts += [ex['prompt'] + ' ' + ex.get('response', '') for ex in chat_ds]\n",
    "random.shuffle(combined_texts)\n",
    "print('Total training sequences:', len(combined_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f879dc",
   "metadata": {},
   "source": [
    "## Data loader that streams activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, texts, model, tok, sae_hook, max_len=256):\n",
    "        self.texts, self.model, self.tok, self.sae_hook = texts, model, tok, sae_hook\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        toks = self.tok(text, return_tensors='pt', truncation=True, max_length=self.max_len).to(device)\n",
    "        # run model up to layer 19 & fetch resid post activations\n",
    "        hooked = HookedSAETransformer(self.model)\n",
    "        with torch.no_grad():\n",
    "            _, cache = hooked.run_with_cache(toks['input_ids'])\n",
    "        acts = cache[self.sae_hook].detach().float()\n",
    "        return acts, acts  # target is same as input for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04247cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ActivationDataset(combined_texts, r1_model, tokenizer, sae_id)\n",
    "loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "print('Activation dataset ready. Batches:', len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa8036",
   "metadata": {},
   "source": [
    "## Fine‑tune loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec144ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.train()\n",
    "opt = torch.optim.AdamW(sae.parameters(), lr=1e-4)\n",
    "epochs = 1  # increase as compute allows\n",
    "for ep in range(epochs):\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        recon = sae(x)\n",
    "        loss = torch.nn.functional.mse_loss(recon, y)\n",
    "        encoded = sae.encoder(x)\n",
    "        loss += 1e-5 * torch.mean(torch.abs(encoded))  # sparsity L1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch {ep+1} batch {i}: loss {loss.item():.6f}')\n",
    "    print(f'Epoch {ep+1} avg loss {total/len(loader):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616bd2",
   "metadata": {},
   "source": [
    "## Evaluate post‑fine‑tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da403d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()\n",
    "sample_prompt = 'Solve 12 * (7 + 5) and explain your steps.'\n",
    "toks = tokenizer(sample_prompt, return_tensors='pt').to(device)\n",
    "hooked_r1 = HookedSAETransformer(r1_model)\n",
    "_, cache_eval = hooked_r1.run_with_cache(toks['input_ids'], saes=[sae])\n",
    "acts = cache_eval[sae_id]\n",
    "recon = cache_eval[f'SAE_RECON:{sae_id}']\n",
    "mse = ((recon - acts)**2).mean().item()\n",
    "print('Post‑tune reconstruction MSE:', mse)\n",
    "\n",
    "# count active features in last token\n",
    "feat = cache_eval[f'SAE:{sae_id}'][-1]\n",
    "active = (feat.abs() > 1e-6).sum().item()\n",
    "print('Active features last token:', active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapted SAE\n",
    "sae.save_to_disk('R1_distill_finetuned_SAE.pth')\n",
    "print('SAE saved to disk.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
