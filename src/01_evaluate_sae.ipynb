{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467e8434",
   "metadata": {},
   "source": [
    "# 01 ‑ Evaluate Goodfire SAE on LLaMA 3.1‑8B vs DeepSeek R1‑Distill\n",
    "This notebook loads Goodfire’s sparse auto‑encoder (layer 19) and:\n",
    "1. Tests it on the original *LLaMA 3.1‑8B Instruct* model.\n",
    "2. Runs the same SAE on *DeepSeek‑R1‑Distill‑Llama‑8B* (without fine‑tuning).\n",
    "3. Visualises reconstruction error and top feature activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libs (run once)\n",
    "!pip install -q sae-lens transformers accelerate datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eca3d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/Users/anton/Library/Mobile Documents/com~apple~CloudDocs/Projects/r1_llama8B_sae/.venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch, matplotlib.pyplot as plt, numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- load models ---\n",
    "base_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "r1_model_name   = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map='auto')\n",
    "base_model.eval()\n",
    "r1_model = AutoModelForCausalLM.from_pretrained(r1_model_name, device_map='auto')\n",
    "r1_model.eval()\n",
    "\n",
    "# --- load SAE ---\n",
    "sae_repo = 'Goodfire/Llama-3.1-8B-Instruct-SAE-l19'\n",
    "sae_id   = 'blocks.19.hook_resid_post'\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(release=sae_repo, sae_id=sae_id, device=device)\n",
    "print('Loaded SAE with latent dim', sae_cfg['d_sae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256a00e",
   "metadata": {},
   "source": [
    "## Evaluate on the original model (LLaMA 3.1‑8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e226ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    'Alice and Bob are planning a party. Alice has 3 apples, Bob brings 5 more. '\n",
    "    'How many apples do they have in total? Explain step by step.'\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "hooked_base = HookedSAETransformer(base_model)\n",
    "_, cache_base = hooked_base.run_with_cache(inputs['input_ids'], saes=[sae])\n",
    "\n",
    "acts_base  = cache_base[sae_id]\n",
    "recon_base = cache_base[f'SAE_RECON:{sae_id}']\n",
    "feat_base  = cache_base[f'SAE:{sae_id}']\n",
    "\n",
    "mse_base = ((recon_base - acts_base) ** 2).mean(dim=-1).cpu().numpy()\n",
    "print('Average reconstruction MSE (base):', mse_base.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise reconstruction error per token\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(mse_base, marker='o')\n",
    "plt.title('LLaMA 3.1‑8B reconstruction error per token')\n",
    "plt.xlabel('Token index')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top‑5 features for final token (base)\n",
    "final_idx = feat_base.shape[0] - 1\n",
    "vals = feat_base[final_idx].detach().cpu().numpy()\n",
    "top = np.argsort(vals)[::-1][:5]\n",
    "for i in top:\n",
    "    print(f'Feature {i}: {vals[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06491ff2",
   "metadata": {},
   "source": [
    "## Evaluate on DeepSeek R1‑Distill (no fine‑tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_r1 = HookedSAETransformer(r1_model)\n",
    "_, cache_r1 = hooked_r1.run_with_cache(inputs['input_ids'], saes=[sae])\n",
    "acts_r1   = cache_r1[sae_id]\n",
    "recon_r1  = cache_r1[f'SAE_RECON:{sae_id}']\n",
    "feat_r1   = cache_r1[f'SAE:{sae_id}']\n",
    "mse_r1 = ((recon_r1 - acts_r1) ** 2).mean(dim=-1).cpu().numpy()\n",
    "print('Average reconstruction MSE (R1‑distill, pre‑tune):', mse_r1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46195584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare error curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(mse_base, label='Base', marker='o')\n",
    "plt.plot(mse_r1,  label='R1‑Distill', marker='s')\n",
    "plt.legend()\n",
    "plt.title('Reconstruction error per token: Base vs R1‑Distill')\n",
    "plt.xlabel('Token index')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02518b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top‑5 features for final token on R1‑distill\n",
    "vals_r1 = feat_r1[final_idx].detach().cpu().numpy()\n",
    "top_r1 = np.argsort(vals_r1)[::-1][:5]\n",
    "for i in top_r1:\n",
    "    print(f'Feature {i}: {vals_r1[i]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
